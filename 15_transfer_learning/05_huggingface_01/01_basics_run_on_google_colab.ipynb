{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvpTvJ_9oAix"
      },
      "source": [
        "# [Huggingface Pipeline](https://huggingface.co/docs/transformers/en/index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCTQ65jboAi2"
      },
      "source": [
        "Running a Hugging Face pipeline locally on your PC allows you to leverage state-of-the-art models for a variety of tasks, such as text classification, question answering, text generation, and more, without the need for deep knowledge in natural language processing or machine learning model architectures. Here's a step-by-step guide to get you started:\n",
        "\n",
        "The first time you run a pipeline for a specific model, the `transformers` library will download the necessary model and tokenizer files, which might take some time depending on your internet connection. Subsequent uses of the same model will be much faster since the model will be cached locally.\n",
        "\n",
        "### Considerations:\n",
        "\n",
        "- **Model Download Size:** Some models can be quite large, requiring significant disk space and bandwidth to download.\n",
        "- **Computational Resources:** Running large models, especially for tasks like text generation or deep learning-based analysis, can be resource-intensive. Ensure your PC has adequate memory and, if supported, a compatible GPU to accelerate computations.\n",
        "- **Environment Management:** Consider using a virtual environment (e.g., via `venv` or `conda`) to manage dependencies and avoid conflicts between different projects.\n",
        "\n",
        "By following these steps, you'll be able to run Hugging Face pipelines locally on your PC, enabling access to a wide range of pre-trained models for various natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_icsCjzEoAi6"
      },
      "source": [
        "## Install the following libraries\n",
        "`!pip install transformers`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaZPSA5goAi7",
        "outputId": "49fc3d17-9405-4ce5-c996-eea9bb3759d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a pipeline for sentiment analysis\n",
        "classifier = pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVJux6-zoAi-",
        "outputId": "e195ea8e-ee98-412a-e82d-225e9e903960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9996317625045776}]\n"
          ]
        }
      ],
      "source": [
        "# Use the pipeline to classify the sentiment of a sentence\n",
        "result = classifier(\"I love using natural language processing tools!\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmYUWRW0oAi_",
        "outputId": "0b2ae613-c958-4bcb-dce6-0811f23c7f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"In this course, we will teach you how to create a full web application using PHP, HTML and CSS, using Laravel's Bootstrap framework to handle requests. In this course, we will demonstrate basic PHP features built in the Laravel framework.\"}]\n"
          ]
        }
      ],
      "source": [
        "# explore text generation\n",
        "generator = pipeline('text-generation')\n",
        "result = generator(\"In this course, we will teach you how to\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ebtjO0oAi_",
        "outputId": "86016ca3-ea63-4ac8-fb88-c99bcae24e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9824061989784241, 'start': 26, 'end': 53, 'answer': 'Natural Language Processing'}\n"
          ]
        }
      ],
      "source": [
        "# question answering\n",
        "question_answerer = pipeline('question-answering')\n",
        "context = \"The name of the course is Natural Language Processing\"\n",
        "result = question_answerer(question=\"What is the name of the course to learn Generative AI?\", context=context)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5YqaeqxoAjA",
        "outputId": "88e1ca47-9e36-40f3-ba6f-378e167e4f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'I-PER', 'score': np.float32(0.9993344), 'index': 13, 'word': 'Ahmad', 'start': 52, 'end': 57}, {'entity': 'I-PER', 'score': np.float32(0.9995565), 'index': 14, 'word': 'A', 'start': 58, 'end': 59}, {'entity': 'I-PER', 'score': np.float32(0.9817724), 'index': 15, 'word': '##z', 'start': 59, 'end': 60}, {'entity': 'I-PER', 'score': np.float32(0.9833365), 'index': 16, 'word': '##har', 'start': 60, 'end': 63}]\n"
          ]
        }
      ],
      "source": [
        "# ner (named entity recognition) pipeline\n",
        "ner = pipeline('ner')\n",
        "result = ner(\"I am ready for the coding,you ready for coding with Ahmad Azhar \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNsPED7SoAjA",
        "outputId": "bcc77388-60bc-4802-f355-fbe5ea143538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I-PER : Ahmad\n",
            "I-PER : A\n",
            "I-PER : ##z\n",
            "I-PER : ##har\n"
          ]
        }
      ],
      "source": [
        "# print the result in a more readable format\n",
        "for entity in result:\n",
        "    print(f\"{entity['entity']} : {entity['word']}\")\n",
        "\n",
        "# I-PER is the tag for a person's name in the dataset used by the NER pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bveQ-xoAjB"
      },
      "source": [
        "------"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}